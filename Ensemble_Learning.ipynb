{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "->Ensemble Learning is a machine learning technique where multiple models (called base learners or weak learners) are combined to build a stronger and more accurate model.\n",
        "\n",
        "Key Idea:\n",
        "\n",
        "The main idea behind ensemble learning is that a group of weak models can perform better together than any single strong model alone. Each model makes its own predictions, and their outputs are combined (by averaging, voting, or weighting) to produce the final result.\n",
        "\n",
        "This helps to:\n",
        "\n",
        "Reduce errors (bias and variance)\n",
        "\n",
        "Improve accuracy\n",
        "\n",
        "Increase model stability\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose three classifiers predict whether an email is spam:\n",
        "\n",
        "Model 1 → Spam\n",
        "\n",
        "Model 2 → Not Spam\n",
        "\n",
        "Model 3 → Spam\n",
        "\n",
        "By majority voting, the final decision will be Spam — this combined prediction is usually more reliable than any single model.\n",
        "\n",
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "->Meaning:\n",
        "\n",
        "Bagging means Bootstrap Aggregating, where multiple models are trained in parallel using random subsets of data.\n",
        "\n",
        "Boosting means training models sequentially, where each model learns from the mistakes of the previous one.\n",
        "\n",
        "Objective:\n",
        "\n",
        "Bagging reduces variance.\n",
        "\n",
        "Boosting reduces bias.\n",
        "\n",
        "Model Training:\n",
        "\n",
        "In Bagging, all models are trained independently.\n",
        "\n",
        "In Boosting, models are trained one after another.\n",
        "\n",
        "Data Sampling:\n",
        "\n",
        "Bagging uses random samples (with replacement).\n",
        "\n",
        "Boosting gives more weight to wrongly predicted samples.\n",
        "\n",
        "Combining Results:\n",
        "\n",
        "Bagging: Uses voting (for classification) or averaging (for regression).\n",
        "\n",
        "Boosting: Uses weighted voting based on model performance.\n",
        "\n",
        "examples:\n",
        "\n",
        "Bagging: Random Forest\n",
        "\n",
        "Boosting: AdaBoost, Gradient Boosting, XGBoost\n",
        "\n",
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "->Bootstrap sampling is a technique where multiple random samples are drawn with replacement from the original training dataset. Each sample is of the same size as the original dataset, but because of replacement, some data points may appear multiple times, while others may be left out.\n",
        "\n",
        "Role in Bagging (ex:- Random Forest):\n",
        "\n",
        "Creates diversity: Each model (like a decision tree) is trained on a different bootstrap sample, which makes them see different subsets of the data.\n",
        "\n",
        "Reduces variance: Since the models are trained on varied samples, their combined (averaged or voted) predictions become more stable and less prone to overfitting.\n",
        "\n",
        "OOB estimation: The data points not included in a particular sample (called out-of-bag samples) are used to estimate model performance without needing a separate validation set.\n",
        "\n",
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "->Out-of-Bag (OOB) samples are the data points that are not included in a particular bootstrap sample during training in Bagging methods like Random Forest.\n",
        "Since bootstrap sampling is done with replacement, about one-third of the data is typically left out each time — these are the OOB samples.\n",
        "\n",
        "OOB Score – Evaluation:\n",
        "\n",
        "For each model (tree), its OOB samples are used as a test set to predict outcomes.\n",
        "\n",
        "The predictions from all trees for their respective OOB samples are combined to estimate the model’s accuracy.\n",
        "\n",
        "The resulting accuracy is called the OOB score.\n",
        "\n",
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "->Decision Tree :\n",
        "\n",
        "Calculated from how much each feature reduces impurity (like Gini or entropy) in that single tree.\n",
        "\n",
        "Importance depends heavily on the specific training data used.\n",
        "\n",
        "Can be biased toward features with more levels or higher cardinality.\n",
        "\n",
        "Easier to interpret, as it’s just one tree.\n",
        "\n",
        "Random Forest:-\n",
        "\n",
        "Calculated by averaging the feature importance scores from all trees in the forest.\n",
        "\n",
        "Importance is more stable and reliable due to averaging across many trees.\n",
        "\n",
        "Reduces bias since multiple trees are trained on different samples and subsets of features.\n",
        "\n",
        "Harder to interpret directly but gives more accurate overall importance."
      ],
      "metadata": {
        "id": "3iFWcV8fBkH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data =load_breast_cancer()\n",
        "\n",
        "x = data.data\n",
        "y = data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state=1)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "\n",
        "importance = model.feature_importances_\n",
        "\n",
        "df = pd.DataFrame({'features' : data.feature_names,\n",
        "                   'importances' : importance})\n",
        "\n",
        "df = df.sort_values(by = 'importances',ascending=False)\n",
        "\n",
        "print(\"the top 5 features are : \",df.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Uf-i7hSyNT",
        "outputId": "d31803dd-958e-41a2-e7e4-635805e9e777"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the top 5 features are :                  features  importances\n",
            "23            worst area     0.140143\n",
            "20          worst radius     0.130077\n",
            "27  worst concave points     0.123622\n",
            "7    mean concave points     0.108659\n",
            "22       worst perimeter     0.101750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "x = data.data\n",
        "y = data.target\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state=1)\n",
        "\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "model1 = BaggingClassifier(estimator=DecisionTreeClassifier(),n_estimators=100,random_state=1)\n",
        "\n",
        "model2 = DecisionTreeClassifier()\n",
        "\n",
        "model1.fit(x_train,y_train)\n",
        "model2.fit(x_train,y_train)\n",
        "\n",
        "y_pred_bagging = model1.predict(x_test)\n",
        "y_pred_tree = model2.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(\"the accuracy of the bagging classifier is : \",accuracy_score(y_test,y_pred_bagging))\n",
        "print(\"the accuracy of the tree classifier is : \",accuracy_score(y_test,y_pred_tree))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3V4cd5nTnV1",
        "outputId": "66a6f52c-42d0-4443-aa63-8a11e430c581"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the accuracy of the bagging classifier is :  0.9666666666666667\n",
            "the accuracy of the tree classifier is :  0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "\n",
        "x = data.data\n",
        "y = data.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "\n",
        "param_grid = {'max_depth' : [1,2,3,4],\n",
        "              'n_estimators' : [100,200,300]}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model = GridSearchCV(estimator = clf,param_grid=param_grid,cv =5,verbose = 0)\n",
        "\n",
        "model.fit(x_train,y_train)\n",
        "print(\"the best parameters are : \",model.best_params_)\n",
        "\n",
        "y_pred = model.best_estimator_.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(\"the final accuracy are : \",accuracy_score(y_test,y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHbkioy5Y5Xk",
        "outputId": "81dc4370-4786-4342-8a92-65c5a538faa6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the best parameters are :  {'max_depth': 2, 'n_estimators': 100}\n",
            "the final accuracy are :  0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "bagging = BaggingRegressor(n_estimators=100, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bagging)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vMRayIef29B",
        "outputId": "dcbd67ab-3d23-4f74-88ab-7149d8d6fd52"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25592438609899626\n",
            "Mean Squared Error (Random Forest Regressor): 0.2553684927247781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "->1. Choose between Bagging or Boosting\n",
        "\n",
        "Use Bagging (Random Forest) if model shows high variance (overfits easily).\n",
        "\n",
        "Use Boosting (XGBoost/Gradient Boosting) if model shows high bias (underfits).\n",
        "\n",
        "Try both and select the one with better validation accuracy.\n",
        "\n",
        "2. Handle Overfitting\n",
        "\n",
        "Limit tree depth (max_depth).\n",
        "\n",
        "Use cross-validation for tuning.\n",
        "\n",
        "Apply regularization (like learning_rate, min_samples_split).\n",
        "\n",
        "Use early stopping (in boosting).\n",
        "\n",
        "3. Select Base Models\n",
        "\n",
        "Use Decision Tree as base for both bagging and boosting.\n",
        "\n",
        "Can also test Logistic Regression or SVM for stacking.\n",
        "\n",
        "4. Evaluate Performance (Cross-Validation)\n",
        "\n",
        "Use Stratified K-Fold Cross-Validation to keep class balance.\n",
        "\n",
        "Measure metrics like Accuracy, AUC-ROC, and F1-Score.\n",
        "\n",
        "Choose model with best mean CV score.\n",
        "\n",
        "5. How Ensemble Improves Decision-Making\n",
        "\n",
        "Combines multiple models → reduces error.\n",
        "\n",
        "Gives more stable and accurate loan default predictions.\n",
        "\n",
        "Helps identify high-risk customers better → fewer bad loans and losses.\n",
        "\n",
        "Improves fairness and confidence in credit decisions."
      ],
      "metadata": {
        "id": "JAx6x0W_qGJd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "46sqNrPUrxnX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}